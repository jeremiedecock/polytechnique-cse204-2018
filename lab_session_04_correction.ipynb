{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "meta",
     "toc_en"
    ],
    "toc-hr-collapsed": false
   },
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "<img src=\"logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSE204-2018](https://moodle.polytechnique.fr/course/view.php?id=6784) Lab session #04\n",
    "\n",
    "Jérémie DECOCK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeremiedecock/polytechnique-cse204-2018/blob/master/lab_session_04_correction.ipynb\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://mybinder.org/v2/gh/jeremiedecock/polytechnique-cse204-2018/master?filepath=lab_session_04_correction.ipynb\"><img align=\"left\" src=\"https://mybinder.org/badge.svg\" alt=\"Open in Binder\" title=\"Open and Execute in Binder\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab session, we have used a **parametric models** to solve **regression problems**.\n",
    "Today you will continue the exploration of regression methods (both parametric and non-parametric)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kernel regression\n",
    "- Weighted Least Squares\n",
    "- Local Linear Regression\n",
    "- Pathological cases\n",
    "- Regularization with Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: there are some differences in notations with the lecture slides. For instance, parameters are noted $w$ in lectures but they are noted $\\theta$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and tool functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_1d_polynomial_regression_samples(n_samples = 15):\n",
    "\n",
    "    #x = np.random.uniform(low=0., high=10., size=n_samples)\n",
    "    x = np.random.uniform(low=0., high=1.5, size=n_samples)\n",
    "\n",
    "    #y = 3. - 2. * x + x ** 2 - x ** 3 + np.random.normal(scale=10., size=x.shape)\n",
    "    y = np.cos(2. * np.pi * x) + np.random.normal(scale=0.1, size=x.shape)\n",
    "\n",
    "    df = pd.DataFrame(np.array([x, y]).T, columns=['x', 'y'])\n",
    "\n",
    "    df = sklearn.utils.shuffle(df).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_1d_regression_samples(dataframe, model=None):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    df = dataframe  # make an alias\n",
    "    \n",
    "    ERROR_MSG1 = \"The `dataframe` parameter should be a Pandas DataFrame having the following columns: ['x', 'y']\"\n",
    "    assert df.columns.values.tolist() == ['x', 'y'], ERROR_MSG1\n",
    "    \n",
    "    if model is not None:\n",
    "        \n",
    "        # Compute the model's prediction\n",
    "        \n",
    "        x_pred = np.linspace(df.x.min(), df.x.max(), 100).reshape(-1, 1)\n",
    "        y_pred = model.predict(x_pred)\n",
    "        \n",
    "        df_pred = pd.DataFrame(np.array([x_pred.flatten(), y_pred.flatten()]).T, columns=['x', 'y'])\n",
    "        \n",
    "        df_pred.plot(x='x', y='y', style='r--', ax=ax)\n",
    "\n",
    "    # Plot also the training points\n",
    "    \n",
    "    df.plot.scatter(x='x', y='y', ax=ax)\n",
    "    \n",
    "    delta_y = df.y.max() - df.y.min()\n",
    "    \n",
    "    plt.ylim((df.y.min() - 0.15 * delta_y,\n",
    "              df.y.max() + 0.15 * delta_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_1d(X, y, theta=None, x_min=0, x_max=2):\n",
    "    assert X.ndim == 2 and X.shape[1] == 2, X.shape\n",
    "    assert y.ndim == 2 and y.shape[1] == 1, y.shape\n",
    "    if theta is not None:\n",
    "        assert theta.ndim == 2 and theta.shape == (2, 1), theta.shape\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:,1], y)\n",
    "\n",
    "    if theta is not None:\n",
    "        x = np.linspace(x_min, x_max, 50)\n",
    "        y = theta[0] + theta[1] * x\n",
    "\n",
    "        ax.plot(x, y, \"--r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Nadaraya-Watson Kernel Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like k-Nearest Neighbors, *Nadaraya-Watson kernel regression* is a non-parametric model, i.e. decisions are made according to known examples from the *learning set* $\\mathcal{D} = \\{(y^{(i)}, \\boldsymbol{x^{(i)}})\\}_{1 \\leq i \\leq n}$ of $n$ examples and considering a kind of proximity relationship.\n",
    "\n",
    "With k-Nearest Neighbors, decisions are based only on the closest neighbors and others examples are simply ignored.\n",
    "On the contrary, with Kernel Regression all examples $(\\boldsymbol{x}^{(i)}, y^{(i)})$ from $\\mathcal{D}$ are used to predict the label $y$ of any new point $\\boldsymbol{x}$: the contribution of $(\\boldsymbol{x}^{(i)}, y^{(i)})$ in this prediction is weighted using a *kernel function* $K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y\n",
    "= f(\\boldsymbol{x})\n",
    "= \\frac{\\sum^{n}_{i=1} K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x}) ~ y^{(i)}}{\\sum^{n}_{i=1} K(\\boldsymbol{x}^{(j)}, \\boldsymbol{x})}\n",
    "= \\sum^{n}_{i=1} y^{(i)} \\omega^{(i)}\n",
    "$$\n",
    "\n",
    "with $\\sum^{n}_{i=1} \\omega^{(i)} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall about the notation used here:\n",
    "- $\\boldsymbol{x}^{(i)}$ is the feature (input) vector of the $i^{\\text{th}}$ example in $\\mathcal{D}$ (and $y^{(i)}$ is its label). Here, $\\boldsymbol{x}^{(i)}$ is not the $i^{\\text{th}}$ power of $\\boldsymbol{x}$ (we will write $\\boldsymbol{x}^{(i)2}$ for the square of $\\boldsymbol{x}^{(i)}$)!\n",
    "- $\\boldsymbol{x}_i$ is the value of $\\boldsymbol{x}$ on the $i^{\\text{th}}$ dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Gaussian kernel $K$ in the following `gaussian_kernel()` Python function.\n",
    "\n",
    "$$\n",
    "K(\\boldsymbol{u}, \\boldsymbol{v})\n",
    "= \\exp\\left(\\frac{-||\\boldsymbol{u} - \\boldsymbol{v}||^2_2}{2 \\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is a parameter equals to $1$ by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can assume $u$ and $v$ to be simple scalars to simplify this Python implementation (i.e. restrict yourself to regression problem with 1 dimension inputs $x \\in \\mathbb{R}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall: $e^x$ is written `math.exp(x)` in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(u, v, sigma = 1.):\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return None   # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Nadaraya-Watson kernel regression in the following `kernel_regression()` Python function.\n",
    "\n",
    "$$\n",
    "\\text{kernel_regression}(\\boldsymbol{x}, \\mathcal{D})\n",
    "= \\frac{\\sum^{n}_{i=1} K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x}) ~ y^{(i)}}{\\sum^{n}_{i=1} K(\\boldsymbol{x}^{(j)}, \\boldsymbol{x})}\n",
    "= y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can assume $x$ to be a simple scalar to simplify the Python implementation.\n",
    "We assume `dataset` to contain examples $(\\boldsymbol{x}^{(i)}, y^{(i)})$ of $\\mathcal{D}$. Here, we assume `dataset` is a Pandas DataFrame having:\n",
    "- one row per example\n",
    "- a column \"x\" containing examples feature (only one dimension here)\n",
    "- a column \"y\" containing examples label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: you can use the following `for` loop to compute $\\sum K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x}) ~ y^{(i)}$: `for xi, yi in zip(df.x, df.y)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_regression(x, dataset):\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return None  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame([[2., 0.],\n",
    "                        [5., 2.],\n",
    "                        [7., 1.],\n",
    "                        [10., 2.],\n",
    "                        [14., 4.],\n",
    "                        [16., 3.],\n",
    "                        [17., 0.]], columns=['x', 'y'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your `kernel_regression()` function with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(0., 20., 200)\n",
    "y_pred = [kernel_regression(x, dataset) for x in x_pred]\n",
    "\n",
    "ax = dataset.plot.scatter(x='x', y='y', label=\"Dataset\", figsize=(12,8))\n",
    "ax.plot(x_pred, y_pred, \"-r\", label=\"Kernel regression\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(u, v, sigma = 1.):\n",
    "    square_dist = (float(u) - float(v))**2\n",
    "    return math.exp( -square_dist / (2. * sigma**2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional check of $K(0, x)$ for $x \\in [-5, 5]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_check = np.linspace(-5., 5., 100)\n",
    "y_check = [gaussian_kernel(0, x) for x in x_check]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(x_check, y_check);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_regression(x, dataset):\n",
    "    df = dataset\n",
    "    w1 = sum([gaussian_kernel(xi, x) * yi for xi, yi in zip(df.x, df.y)])\n",
    "    w2 = sum([gaussian_kernel(xj, x) for xj in df.x])\n",
    "    return w1 / w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(0., 20., 200)\n",
    "y_pred = [kernel_regression(x, dataset) for x in x_pred]\n",
    "\n",
    "ax = dataset.plot.scatter(x='x', y='y', label=\"Dataset\", figsize=(12,8))\n",
    "ax.plot(x_pred, y_pred, \"-r\", label=\"Kernel regression\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## *Weighted Least Squares*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some regression problems, it may be helpful to give different importance to examples in the *learning set* $\\mathcal{D} = \\{(y^{(i)}, \\boldsymbol{x^{(i)}})\\}_{1 \\leq i \\leq n}$ that is to say associate a weight $\\omega^{(i)}$ to example $\\boldsymbol{x}^{(i)}$ in order to prioritize some of them and ignore some others (e.g. outliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introducing these weights in the method of Least Square, the regression problem becomes:\n",
    "\n",
    "$$E(\\boldsymbol{\\theta}) = \\sum_{i=1}^n \\omega^{(i)} (y^{(i)} - \\boldsymbol{x}^{(i)} \\boldsymbol{\\theta})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the matrix notation, we put weights $\\omega_i$ in the diagonal of the following matrix $\\Omega$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Omega =\n",
    "\\begin{pmatrix}\n",
    "\\omega^{(1)} & 0            & \\cdots & 0 \\\\\n",
    "0            & \\omega^{(2)} & \\cdots & 0 \\\\\n",
    "\\vdots       & \\vdots       & \\ddots & \\vdots \\\\\n",
    "0            & 0            & \\cdots & \\omega^{(n)} \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can write:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E(\\boldsymbol{\\theta}) = (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta})^T \\Omega (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with:\n",
    "$$\n",
    "\\boldsymbol{X} = \\begin{pmatrix} 1 & x_1^{(1)} & \\dots & x_p^{(1)} \\\\ \\vdots & \\vdots & \\dots & \\vdots \\\\ 1 & x_1^{(n)} & \\dots & x_p^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{y} = \\begin{pmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(n)} \\end{pmatrix}\n",
    "\\quad \\quad\n",
    "\\boldsymbol{\\theta} = \\begin{pmatrix} \\theta_0 \\\\ \\vdots \\\\ \\theta_p \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a sheet of paper:\n",
    "- Compute the analytic formulation of the gradient $\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})$\n",
    "- Compute the analytic formulation of the optimal parameter $\\boldsymbol{\\theta^*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it a convexe optimization problem like *Ordinary Least Squares* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following dataset and weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1],\n",
    "              [1, 2],\n",
    "              [1, 3],\n",
    "              [1, 4],\n",
    "              [1, 5]])\n",
    "\n",
    "y = np.array([1.8, 4.5, 3.4, 3.6, 4.2]).reshape([-1, 1])\n",
    "\n",
    "Omega = np.diag([1, 2, 3, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following Python implementation of the `weighted_least_squares()` procedure.\n",
    "It should return the optimal parameter $\\boldsymbol{\\theta^*}$ using the method of Least Square for the matrix of weights $\\Omega$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_least_squares(X, Omega, y):\n",
    "    \n",
    "    # TODO\n",
    "\n",
    "    theta = None  # TODO\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = weighted_least_squares(X, Omega, y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we expect $\\theta$ to be a Numpy array of shape `(1, 2)` (i.e. a vector of two elements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy recall:\n",
    "- The transpose of a matrix `X` is obtained with `X.T`\n",
    "- The inverse of a matrix `X` is obtained with `np.linalg.inv(X)`\n",
    "- The product of two matrices `X` and `Y` is obtained with `np.dot(X, Y)`\n",
    "- The dot product of a matrix `X` and a vector `y` is obtained with `np.dot(X, y)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check graphically your model using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_1d(X, y, theta, x_min=0, x_max=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the weights in $\\Omega$ to ignore the second point $x = 2$ (give the same weight to all other points) then recompute $\\theta$ using `weighted_least_squares()` and check the results on plots with `plot_regression_1d()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C.f. lecture #3 slide 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall:\n",
    "\n",
    "$$\n",
    "(\\boldsymbol{A} + \\boldsymbol{B})^T = \\boldsymbol{A}^T + \\boldsymbol{B}^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\boldsymbol{A} \\boldsymbol{B})^T = \\boldsymbol{B}^T \\boldsymbol{A}^T\n",
    "$$\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Associativity:} & \\quad\n",
    "(\\boldsymbol{A} \\boldsymbol{B}) \\boldsymbol{C}\n",
    "= \\boldsymbol{A} (\\boldsymbol{B} \\boldsymbol{C})\n",
    "= \\boldsymbol{A} \\boldsymbol{B} \\boldsymbol{C}\n",
    "\\\\\n",
    "\\text{Non-commutativity:} & \\quad\n",
    "\\boldsymbol{A} \\boldsymbol{B}\n",
    "\\neq \\boldsymbol{B} \\boldsymbol{A}\n",
    "\\\\\n",
    "\\text{Distributivity:} & \\quad\n",
    "\\boldsymbol{A} (\\boldsymbol{B} + \\boldsymbol{C})\n",
    "= \\boldsymbol{A} \\boldsymbol{B} + \\boldsymbol{A} \\boldsymbol{C} \\\\\n",
    "& \\quad\n",
    "(\\boldsymbol{B} + \\boldsymbol{C}) \\boldsymbol{A} \n",
    "= \\boldsymbol{B} \\boldsymbol{A} + \\boldsymbol{C} \\boldsymbol{A}\n",
    "\\end{align}\n",
    "\n",
    "Differential identities for matrices:\n",
    "\n",
    "\\begin{align}\n",
    "d(\\boldsymbol{A}) & = 0  \\quad \\quad \\quad \\text{if } \\boldsymbol{A} \\text{ is not function of } \\boldsymbol{X} \\\\\n",
    "d(a\\boldsymbol{X}) & = a d \\boldsymbol{X}  \\quad \\quad \\text{if } a \\text{ is not function of } \\boldsymbol{X} \\\\\n",
    "d(\\boldsymbol{X} +\\boldsymbol{Y}) & = d \\boldsymbol{X} + d \\boldsymbol{Y} \\\\\n",
    "d(\\boldsymbol{XY}) & = (d \\boldsymbol{X}) \\boldsymbol{Y} + \\boldsymbol{X} (d \\boldsymbol{Y}) \\\\\n",
    "d(\\boldsymbol{X}^T) & = (d \\boldsymbol{X})^T \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets develop $E(\\boldsymbol{\\theta})$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "E(\\boldsymbol{\\theta})\n",
    "& = \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right)^T \\boldsymbol{\\Omega} \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right) \\\\\n",
    "& = \\left( \\boldsymbol{y}^T - (\\boldsymbol{X} \\boldsymbol{\\theta})^T \\right) \\boldsymbol{\\Omega} \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right) \\\\\n",
    "& = \\left( \\boldsymbol{y}^T - (\\boldsymbol{\\theta}^T \\boldsymbol{X}^T) \\right) \\boldsymbol{\\Omega} \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right) \\\\\n",
    "& = \\left( \\boldsymbol{y}^T \\boldsymbol{\\Omega} - \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{\\Omega} \\right) \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right) \\\\\n",
    "& = \\boldsymbol{y}^T \\boldsymbol{\\Omega} \\boldsymbol{y} - \\underbrace{\\boldsymbol{y}^T \\boldsymbol{\\Omega} \\boldsymbol{X} \\boldsymbol{\\theta}}_{\\text{scalar}} - \\underbrace{\\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{\\Omega} \\boldsymbol{y}}_{\\text{scalar}} + \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{\\Omega} \\boldsymbol{X} \\boldsymbol{\\theta} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Since\n",
    "$\\boldsymbol{y}^T \\boldsymbol{\\Omega} \\boldsymbol{X} \\boldsymbol{\\theta}$\n",
    "is a $1 \\times 1$ matrix, or a scalar, and its transpose\n",
    "$(\\boldsymbol{y}^T \\boldsymbol{\\Omega} \\boldsymbol{X} \\boldsymbol{\\theta})^T = (\\boldsymbol{\\Omega} \\boldsymbol{X} \\boldsymbol{\\theta})^T \\boldsymbol{y} = (\\boldsymbol{X} \\boldsymbol{\\theta})^T \\boldsymbol{\\Omega}^T \\boldsymbol{y} = \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{\\Omega}^T \\boldsymbol{y} = \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{\\Omega} \\boldsymbol{y}$\n",
    "is the same scalar ($\\boldsymbol{\\Omega} = \\boldsymbol{\\Omega}^T$ as $\\boldsymbol{\\Omega}$ is a diagonal matrix), then\n",
    "$- \\boldsymbol{y}^T \\boldsymbol{\\Omega} \\boldsymbol{X} \\boldsymbol{\\theta} - \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{\\Omega} \\boldsymbol{y} = -2 \\boldsymbol{y}^T \\boldsymbol{\\Omega} \\boldsymbol{X} \\boldsymbol{\\theta} = -2 \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{\\Omega} \\boldsymbol{y}$.\n",
    "\n",
    "Thus we have:\n",
    "\n",
    "\\begin{align}\n",
    "E(\\boldsymbol{\\theta})\n",
    "& = \\boldsymbol{y}^T \\boldsymbol{\\Omega} \\boldsymbol{y} -2 \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{\\Omega} \\boldsymbol{y} + \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{\\Omega} \\boldsymbol{X} \\boldsymbol{\\theta} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can write $\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})\n",
    "& = \\nabla_{\\boldsymbol{\\theta}} \\left[ \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right)^T \\boldsymbol{\\Omega} \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right) \\right] \\\\\n",
    "& = \\nabla_{\\boldsymbol{\\theta}} \\left[ \\boldsymbol{y}^T \\boldsymbol{\\Omega} \\boldsymbol{y} -2 \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{\\Omega} \\boldsymbol{y} + \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{\\Omega} \\boldsymbol{X} \\boldsymbol{\\theta} \\right] \\\\\n",
    "& = \\nabla_{\\boldsymbol{\\theta}} \\left[ \\boldsymbol{y}^T \\boldsymbol{\\Omega} \\boldsymbol{y} \\right] + \\nabla_{\\boldsymbol{\\theta}} \\left[-2 \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{\\Omega} \\boldsymbol{y} \\right] + \\underbrace{\\nabla_{\\boldsymbol{\\theta}} \\left[ \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{\\Omega} \\boldsymbol{X} \\boldsymbol{\\theta} \\right]}_{\\text{see recall below}} \\\\\n",
    "& = -2 \\boldsymbol{X}^T \\boldsymbol{\\Omega} \\boldsymbol{y} + 2 \\boldsymbol{X}^T \\boldsymbol{\\Omega} \\boldsymbol{X} \\boldsymbol{\\theta} \\\\\n",
    "& = -2 \\boldsymbol{X}^T \\boldsymbol{\\Omega} (\\boldsymbol{y} - \\boldsymbol{X\\theta})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{\\theta}} \\left[\\boldsymbol{\\theta}^T \\boldsymbol{A} \\boldsymbol{\\theta} \\right] = 2 \\boldsymbol{A} \\boldsymbol{\\theta} \\quad \\quad \\text{if } \\boldsymbol{A} \\text{ is not a function of } \\boldsymbol{\\theta} \\text{ and if } \\boldsymbol{A} \\text{ is a symmetric matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can write the formula of the optimal parameter $\\boldsymbol{\\theta^*}$ (the one that minimize $E$ i.e. the one such that $\\nabla_{\\boldsymbol{\\theta^*}} E(\\boldsymbol{\\theta^*}) = 0$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\nabla_{\\boldsymbol{\\theta^*}} E(\\boldsymbol{\\theta^*}) & = 0 \\\\\n",
    "\\Leftrightarrow -2 \\boldsymbol{X}^T \\boldsymbol{\\Omega} (\\boldsymbol{y} - \\boldsymbol{X\\theta^*}) & = 0\\\\\n",
    "\\Leftrightarrow \\boldsymbol{X}^T \\boldsymbol{\\Omega} (\\boldsymbol{y} - \\boldsymbol{X\\theta^*}) & = 0\\\\\n",
    "\\Leftrightarrow  \\boldsymbol{X^T \\Omega y} - \\boldsymbol{X^T \\Omega X\\theta^*} & = 0\\\\\n",
    "\\Leftrightarrow \\boldsymbol{X^T \\Omega X\\theta^*} & = \\boldsymbol{X^T \\Omega y} \\\\\n",
    "\\Leftrightarrow \\boldsymbol{\\theta^*} & = (\\boldsymbol{X^T \\Omega X})^{-1} \\boldsymbol{X}^T \\boldsymbol{\\Omega} \\boldsymbol{y}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it's a convexe optimization problem like *Ordinary Least Squares*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_least_squares(X, Omega, y):\n",
    "    X_Omega_y = np.dot(np.dot(X.T, Omega), y)\n",
    "    X_Omega_X = np.dot(np.dot(X.T, Omega), X)\n",
    "    inv_X_Omega_X = np.linalg.inv(X_Omega_X)\n",
    "\n",
    "    theta = np.dot(inv_X_Omega_X, X_Omega_y)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = weighted_least_squares(X, Omega, y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_1d(X, y, theta, x_min=0, x_max=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Omega = np.diag([1, 0, 1, 1, 1])\n",
    "\n",
    "theta = weighted_least_squares(X, Omega, y)\n",
    "\n",
    "plot_regression_1d(X, y, theta, x_min=0, x_max=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: let's check with another $\\Omega$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Omega = np.diag([0, 0, 1, 1, 0])\n",
    "\n",
    "theta = weighted_least_squares(X, Omega, y)\n",
    "\n",
    "plot_regression_1d(X, y, theta, x_min=0, x_max=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## *Local Linear Regression*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possible usage of *Weighted Least Squares* is the *Local Linear Regression*. It uses a *Kernel* $K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x})$ to define the weight $\\omega^{(i)}$ assigned to example $i$. Thus it's a linear regression giving more importance to examples close to the point $\\boldsymbol{x}$ to predict. This mean that this method does a new fit (in other words it computes a new $\\boldsymbol{\\theta}^*$) for each new point to predict!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each point $\\boldsymbol{x}$ to predict:\n",
    "1. Compute weights $\\omega^{(i)}$ assigned to examples $\\boldsymbol{x}^{(i)}$ w.r.t their distance to $\\boldsymbol{x}$: $\\omega^{(i)} = K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x})$\n",
    "2. Fit Weighted Least Squares to obtain the $\\boldsymbol{\\theta}^*$ vector associated to $\\boldsymbol{x}$\n",
    "3. Return the prediction $y = \\boldsymbol{x\\theta}^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = gen_1d_polynomial_regression_samples(n_samples=30)\n",
    "\n",
    "plot_1d_regression_samples(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following Python implementation of the `locally_weighted_regression()` procedure defined above.\n",
    "It should use the previously implemented `gaussian_kernel()` function (with `sigma=0.1`) and `weighted_least_squares()` function. It should return the predicted label $y$ corresponding to the input $\\boldsymbol{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locally_weighted_regression(x, dataset, sigma=0.1):\n",
    "    df = dataset   # Make an alias\n",
    "    \n",
    "    # Compute a weight wi for each example xi of the dataset: the closer xi is to x, the smaller wi is\n",
    "    \n",
    "    #wi = ...                                      # <- **TODO: UNCOMMENT AND COMPLETE**\n",
    "    #Omega = ...                                   # <- **TODO: UNCOMMENT AND COMPLETE**\n",
    "    \n",
    "    # Fit weighted least squares to obtain theta ##\n",
    "    \n",
    "    intercept = np.ones(shape=len(df.x))\n",
    "    X = np.array([intercept, df.x]).T\n",
    "    y = df.y.values.reshape([-1, 1])\n",
    "    #theta = weighted_least_squares(X, Omega, y)   # <- **TODO: UNCOMMENT**\n",
    "    \n",
    "    # Return prediction y = f(x) ##################\n",
    "    \n",
    "    # ...                                          # <- **TODO: UNCOMMENT AND COMPLETE**\n",
    "    y = 0                                          # <- **TODO: UPDATE**\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(0., 1.5, 100)\n",
    "y_pred = [locally_weighted_regression(x, dataset, sigma=0.1) for x in x_pred]\n",
    "\n",
    "ax = dataset.plot.scatter(x='x', y='y', figsize=(16, 8))\n",
    "ax.plot(x_pred, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happen when you change the value of the variable `sigma` parameter (try e.g. `sigma=0.3`)?\n",
    "Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use Local Linear Regression to forecast time series as it was asked in the exercise 7 of the last tutorial? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    URL = \"natural_gas_co2_emissions_for_electric_power_sector.csv\"\n",
    "    df = pd.read_csv(URL, parse_dates=[0])\n",
    "except Exception:\n",
    "    URL = \"https://raw.githubusercontent.com/jeremiedecock/polytechnique-cse204-2018/master/natural_gas_co2_emissions_for_electric_power_sector.csv\"\n",
    "    df = pd.read_csv(URL, parse_dates=[0])\n",
    "\n",
    "df = pd.read_csv(URL, parse_dates=[0])\n",
    "\n",
    "df['x'] = df.index\n",
    "df['y'] = df.co2_emissions\n",
    "\n",
    "df[['x','y']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = gen_1d_polynomial_regression_samples(n_samples=30)\n",
    "\n",
    "plot_1d_regression_samples(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locally_weighted_regression(x, dataset, sigma=0.1):\n",
    "    df = dataset   # Make an alias\n",
    "    \n",
    "    # Compute a weight wi for each example xi of the dataset: the closer xi is to x, the smaller wi is\n",
    "    wi = [gaussian_kernel(xi, x, sigma) for xi in df.x]\n",
    "    Omega = np.diag(wi)\n",
    "    \n",
    "    # Fit weighted least squares to obtain theta\n",
    "    intercept = np.ones(shape=len(df.x))\n",
    "    X = np.array([intercept, df.x]).T\n",
    "    y = df.y.values.reshape([-1, 1])\n",
    "    theta = weighted_least_squares(X, Omega, y)\n",
    "    \n",
    "    # Return prediction y = f(x)\n",
    "    y = theta[0] + theta[1] * x\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(0., 1.5, 100)\n",
    "y_pred = [locally_weighted_regression(x, dataset, sigma=0.1) for x in x_pred]\n",
    "\n",
    "ax = dataset.plot.scatter(x='x', y='y', figsize=(16, 8))\n",
    "ax.plot(x_pred, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(0., 1.5, 100)\n",
    "y_pred = [locally_weighted_regression(x, dataset, sigma=0.3) for x in x_pred]\n",
    "\n",
    "ax = dataset.plot.scatter(x='x', y='y', figsize=(16, 8))\n",
    "ax.plot(x_pred, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method has poor performance when the new point to predict is located outside the area known examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    URL = \"natural_gas_co2_emissions_for_electric_power_sector.csv\"\n",
    "    df = pd.read_csv(URL, parse_dates=[0])\n",
    "except Exception:\n",
    "    URL = \"https://raw.githubusercontent.com/jeremiedecock/polytechnique-cse204-2018/master/natural_gas_co2_emissions_for_electric_power_sector.csv\"\n",
    "    df = pd.read_csv(URL, parse_dates=[0])\n",
    "\n",
    "df = pd.read_csv(URL, parse_dates=[0])\n",
    "\n",
    "df['x'] = df.index\n",
    "df['y'] = df.co2_emissions\n",
    "\n",
    "df[['x','y']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.linspace(0., len(df) + 5, 1000)\n",
    "y_pred = [locally_weighted_regression(x, df, sigma=0.5) for x in x_pred]\n",
    "\n",
    "ax = df.plot.scatter(x='x', y='y', figsize=(16, 8))\n",
    "ax.plot(x_pred, y_pred, \"-r\", label=\"Prediction\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Pathological cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following implementation of the least squares method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(X, y):\n",
    "    XX = np.dot(X.T, X)\n",
    "    Xy = np.dot(X.T, y)\n",
    "    invXX = np.linalg.inv(XX)\n",
    "\n",
    "    theta = np.dot(invXX, Xy)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use it to apply linear regularization to some datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is wrong with the following dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1.1, 2],\n",
    "              [2, 2.2, 4]])\n",
    "y = np.array([1.8, 2.7])\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#theta = least_squares(X, y)   # <- **TODO: UNCOMMENT**\n",
    "#theta                         # <- **TODO: UNCOMMENT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is wrong with the following dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2, 3, 4, 5], [2, 4, 6, 8, 10]]).T\n",
    "y = np.array([1.8, 2.7, 3.4, 3.8, 3.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#theta = least_squares(X, y)   # <- **TODO: UNCOMMENT**\n",
    "#theta                         # <- **TODO: UNCOMMENT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary Least Squares requires to invert $\\boldsymbol{X^TX}$.\n",
    "\n",
    "But here we have $d > n$ i.e. we have greater input dimensionality $d$ than examples $n$. Thus there are no independent columns in $\\boldsymbol{X}$ and the matrix $\\boldsymbol{X^TX}$ is non-invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have colinearity: the matrix $\\boldsymbol{X^TX}$ is not full rank, determinant is zero and thus it's non-invertible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: here $\\boldsymbol{x}$ has two dimensions here. Actually a 3D plot $(x_1, x_2, y)$ would show this colinearity more efficiently..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Regularization with Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2.5, 3, 4.2, 5.5])\n",
    "y = np.array([3.1, 3.5, 6.8, 10.9, 12.3])\n",
    "\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply basis expansion to fit a polynomial model on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basis_expansion(x, degree=5):\n",
    "    Z_list = [np.ones(shape=x.shape)]   # intercept\n",
    "    \n",
    "    for deg_index in range(1, degree + 1):\n",
    "        Z_list.append(x**deg_index)\n",
    "    \n",
    "    return np.array(Z_list).T\n",
    "\n",
    "DEGREE = 7\n",
    "\n",
    "Z = basis_expansion(x, degree=DEGREE)\n",
    "\n",
    "# Make and fit the model\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "model.fit(Z, y)\n",
    "\n",
    "print(\"Coefs:\", model.coef_)\n",
    "\n",
    "# Compute the model's prediction\n",
    "\n",
    "x_pred = np.linspace(x.min(), x.max(), 100)\n",
    "Z_pred = basis_expansion(x_pred, degree=DEGREE)\n",
    "Z_pred\n",
    "\n",
    "y_pred = model.predict(Z_pred)\n",
    "\n",
    "# Plot prediction and training set\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "ax.plot(x_pred, y_pred)\n",
    "ax.scatter(x, y)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a polynomial function of degree 7 is certainly not adapter to fit efficiently these data. Here we have a clear over-fitting: the model is too complex for the data and it will have poor generalization performance (i.e. big error on new unknown data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A solution is to reduce the complexity of the model using a lower polynomial degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative is to apply a *regularization method* like the *ridge regularization* (a.k.a. *L2 regularization*) which applies a penalty on the value of $\\theta$ to constrain it to be as small as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A $\\boldsymbol{\\theta}$ with small elements usually make the model simpler and bring better generalization performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This L2 regularization is included in the least square method as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{\\theta}^*\n",
    "\\leftarrow \\arg\\min_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})\n",
    "\\quad \\text{with} \\quad\n",
    "E(\\boldsymbol{\\theta})\n",
    "= \\underbrace{||\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta}||^2_2}_{\\text{error term}} ~~ + \\underbrace{\\lambda ||\\boldsymbol{\\theta}||^2_2}_{\\text{regularization}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\lambda \\in \\mathbb{R}^+$ is the *regularization strength* coefficient:\n",
    "- when $\\lambda$ goes to infinity, the regularization term dominates the error term (MSE) and the coefficients $\\boldsymbol{\\theta}$ tend to zero\n",
    "- when $\\lambda$ goes to 0, the regularization term loose in importance and eventually the regularization term is ignored\n",
    "- $\\lambda$ is a *meta parameter*\n",
    "- the best $\\lambda$ for a problem can be computed empirically or automatically (e.g. grid search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a sheet of paper:\n",
    "- Compute the analytic formulation of the gradient $\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})$\n",
    "- Compute the analytic formulation of the optimal parameter $\\boldsymbol{\\theta^*}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it a convexe optimization problem like *Ordinary Least Squares* ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the following Scikit Learn implementation of the Ridge Regression (more info here: https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.Ridge(alpha=6., fit_intercept=False)\n",
    "\n",
    "model.fit(Z, y)\n",
    "\n",
    "coefs = [model.intercept_] + model.coef_\n",
    "\n",
    "print(\"Coefs:\", coefs)\n",
    "\n",
    "# Compute the model's prediction\n",
    "\n",
    "x_pred = np.linspace(x.min(), x.max(), 100)\n",
    "Z_pred = basis_expansion(x_pred, degree=DEGREE)\n",
    "Z_pred\n",
    "\n",
    "y_pred = model.predict(Z_pred)\n",
    "\n",
    "# Plot prediction and training set\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "ax.plot(x_pred, y_pred)\n",
    "ax.scatter(x, y);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the value of the `alpha` parameter in `sklearn.linear_model.Ridge` and explain what happen (in Scikit Learn the $\\lambda$ regression strength is named $\\alpha$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Ridge coefficients as a function of the regularization parameter.\n",
    "\n",
    "Evaluate the following sequence of regularization strength: `alphas = np.logspace(-2, 5, 50)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the following function to implement the ridge regression in Python (without using Scikit Learn). Check it as in question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X, y):\n",
    "    XX = np.dot(X.T, X)\n",
    "    Xy = np.dot(X.T, y)\n",
    "    invXX = np.linalg.inv(XX)\n",
    "\n",
    "    theta = np.dot(invXX, Xy)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall:\n",
    "\n",
    "$$\n",
    "(\\boldsymbol{A} + \\boldsymbol{B})^T = \\boldsymbol{A}^T + \\boldsymbol{B}^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\boldsymbol{A} \\boldsymbol{B})^T = \\boldsymbol{B}^T \\boldsymbol{A}^T\n",
    "$$\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Associativity:} & \\quad\n",
    "(\\boldsymbol{A} \\boldsymbol{B}) \\boldsymbol{C}\n",
    "= \\boldsymbol{A} (\\boldsymbol{B} \\boldsymbol{C})\n",
    "= \\boldsymbol{A} \\boldsymbol{B} \\boldsymbol{C}\n",
    "\\\\\n",
    "\\text{Non-commutativity:} & \\quad\n",
    "\\boldsymbol{A} \\boldsymbol{B}\n",
    "\\neq \\boldsymbol{B} \\boldsymbol{A}\n",
    "\\\\\n",
    "\\text{Distributivity:} & \\quad\n",
    "\\boldsymbol{A} (\\boldsymbol{B} + \\boldsymbol{C})\n",
    "= \\boldsymbol{A} \\boldsymbol{B} + \\boldsymbol{A} \\boldsymbol{C} \\\\\n",
    "& \\quad\n",
    "(\\boldsymbol{B} + \\boldsymbol{C}) \\boldsymbol{A} \n",
    "= \\boldsymbol{B} \\boldsymbol{A} + \\boldsymbol{C} \\boldsymbol{A}\n",
    "\\end{align}\n",
    "\n",
    "Differential identities for matrices:\n",
    "\n",
    "\\begin{align}\n",
    "d(\\boldsymbol{A}) & = 0  \\quad \\quad \\quad \\text{if } \\boldsymbol{A} \\text{ is not function of } \\boldsymbol{X} \\\\\n",
    "d(a\\boldsymbol{X}) & = a d \\boldsymbol{X}  \\quad \\quad \\text{if } a \\text{ is not function of } \\boldsymbol{X} \\\\\n",
    "d(\\boldsymbol{X} +\\boldsymbol{Y}) & = d \\boldsymbol{X} + d \\boldsymbol{Y} \\\\\n",
    "d(\\boldsymbol{XY}) & = (d \\boldsymbol{X}) \\boldsymbol{Y} + \\boldsymbol{X} (d \\boldsymbol{Y}) \\\\\n",
    "d(\\boldsymbol{X}^T) & = (d \\boldsymbol{X})^T \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets develop $E(\\boldsymbol{\\theta})$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "E(\\boldsymbol{\\theta})\n",
    "& = \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right)^T \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right) + \\lambda (\\boldsymbol{\\theta}^T \\boldsymbol{\\theta}) \\\\\n",
    "& = \\left( \\boldsymbol{y}^T - (\\boldsymbol{X} \\boldsymbol{\\theta})^T \\right) \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right) + \\lambda \\boldsymbol{\\theta}^T \\boldsymbol{\\theta} \\\\\n",
    "& = \\left( \\boldsymbol{y}^T - (\\boldsymbol{\\theta}^T \\boldsymbol{X}^T) \\right) \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right) + \\lambda \\boldsymbol{\\theta}^T \\boldsymbol{\\theta} \\\\\n",
    "& = \\boldsymbol{y}^T \\boldsymbol{y} - \\boldsymbol{y}^T \\boldsymbol{X} \\boldsymbol{\\theta} - (\\boldsymbol{\\theta}^T \\boldsymbol{X}^T) \\boldsymbol{y} + (\\boldsymbol{\\theta}^T \\boldsymbol{X}^T) (\\boldsymbol{X} \\boldsymbol{\\theta}) + \\lambda \\boldsymbol{\\theta}^T \\boldsymbol{\\theta} \\\\\n",
    "& = \\boldsymbol{y}^T \\boldsymbol{y} - \\underbrace{\\boldsymbol{y}^T \\boldsymbol{X} \\boldsymbol{\\theta}}_{\\text{scalar}} - \\underbrace{\\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{y}}_{\\text{scalar}} + \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{\\theta} + \\lambda \\boldsymbol{\\theta}^T \\boldsymbol{\\theta} \\\\\n",
    "\\end{align}\n",
    "\n",
    "Since $\\boldsymbol{y}^T \\boldsymbol{X} \\boldsymbol{\\theta}$ is a $1 \\times 1$ matrix, or a scalar, and its transpose $(\\boldsymbol{y}^T \\boldsymbol{X} \\boldsymbol{\\theta})^T = (\\boldsymbol{X} \\boldsymbol{\\theta})^T \\boldsymbol{y} = \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{y}$ is the same scalar, then $- \\boldsymbol{y}^T \\boldsymbol{X} \\boldsymbol{\\theta} - \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{y} = -2 \\boldsymbol{y}^T \\boldsymbol{X} \\boldsymbol{\\theta} = -2 \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{y}$.\n",
    "\n",
    "Thus we have:\n",
    "\n",
    "\\begin{align}\n",
    "E(\\boldsymbol{\\theta})\n",
    "& = \\boldsymbol{y}^T \\boldsymbol{y} \\underbrace{-2 \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{y}} + \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{\\theta} + \\lambda \\boldsymbol{\\theta}^T \\boldsymbol{\\theta} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can write $\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta})\n",
    "& = \\nabla_{\\boldsymbol{\\theta}} \\left[ \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right)^T \\left( \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\theta} \\right)  + \\lambda (\\boldsymbol{\\theta}^T \\boldsymbol{\\theta}) \\right] \\\\\n",
    "& = \\nabla_{\\boldsymbol{\\theta}} \\left[ \\boldsymbol{y}^T \\boldsymbol{y} -2 \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{y} + \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{\\theta}  + \\lambda \\boldsymbol{\\theta}^T \\boldsymbol{\\theta} \\right] \\\\\n",
    "& = \\nabla_{\\boldsymbol{\\theta}} \\left[ \\boldsymbol{y}^T \\boldsymbol{y} \\right] + \\nabla_{\\boldsymbol{\\theta}} \\left[-2 \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{y} \\right] + \\underbrace{\\nabla_{\\boldsymbol{\\theta}} \\left[ \\boldsymbol{\\theta}^T \\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{\\theta} \\right]}_{\\text{see recall below}} + \\nabla_{\\boldsymbol{\\theta}} \\left[\\lambda \\boldsymbol{\\theta}^T \\boldsymbol{\\theta} \\right] \\\\\n",
    "& = -2 \\boldsymbol{X}^T \\boldsymbol{y} + 2 \\boldsymbol{X}^T \\boldsymbol{X} \\boldsymbol{\\theta} + 2\\lambda \\boldsymbol{I}_p \\boldsymbol{\\theta} \\\\\n",
    "& = \\left( \\lambda \\boldsymbol{I}_p + \\boldsymbol{X}^T \\boldsymbol{X} \\right) 2\\boldsymbol{\\theta} - 2\\boldsymbol{X}^T \\boldsymbol{y}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\boldsymbol{I}_p \\in \\mathbb{R}^{p \\times p}$ is the identity matrix of order $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\boldsymbol{\\theta}^T \\boldsymbol{A} \\boldsymbol{\\theta}}{\\partial \\boldsymbol{\\theta}} = 2 \\boldsymbol{A} \\boldsymbol{\\theta} \\quad \\quad \\text{if } \\boldsymbol{A} \\text{ is not a function of } \\boldsymbol{\\theta} \\text{ and if } \\boldsymbol{A} \\text{ is a symmetric matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can write the formula of the optimal parameter $\\boldsymbol{\\theta^*}$ (the one that minimize $E(\\boldsymbol{\\theta})$ i.e. the one such that $\\nabla_{\\boldsymbol{\\theta}} E(\\boldsymbol{\\theta}) = 0$):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\nabla_{\\boldsymbol{\\theta^*}} E(\\boldsymbol{\\theta^*}) & = 0 \\\\\n",
    "\\Leftrightarrow \\left( \\lambda \\boldsymbol{I}_p + \\boldsymbol{X}^T \\boldsymbol{X} \\right) 2\\boldsymbol{\\theta^*} - 2\\boldsymbol{X}^T \\boldsymbol{y}\n",
    " & = 0\\\\\n",
    "\\Leftrightarrow \\left( \\lambda \\boldsymbol{I}_p + \\boldsymbol{X}^T \\boldsymbol{X} \\right) \\boldsymbol{\\theta^*} & = \\boldsymbol{X}^T \\boldsymbol{y}\\\\\n",
    "\\Leftrightarrow \\boldsymbol{\\theta^*} & = \\left( \\lambda \\boldsymbol{I}_p + \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $\\lambda > 0$, the matrix $\\lambda \\boldsymbol{I}_p + \\boldsymbol{X}^T \\boldsymbol{X}$ is always invertible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it's a convexe optimization problem like *Ordinary Least Squares*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E(\\boldsymbol{\\theta})$ is a quadratic form (convex function) thus it has a unique global minimum $\\boldsymbol{\\theta^*}$ where $\\nabla_{\\boldsymbol{\\theta^*}} E(\\boldsymbol{\\theta^*}) = \\boldsymbol{0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.Ridge(alpha=6., fit_intercept=False)\n",
    "\n",
    "model.fit(Z, y)\n",
    "\n",
    "coefs = [model.intercept_] + model.coef_\n",
    "\n",
    "print(\"Coefs:\", coefs)\n",
    "\n",
    "# Compute the model's prediction\n",
    "\n",
    "x_pred = np.linspace(x.min(), x.max(), 100)\n",
    "Z_pred = basis_expansion(x_pred, degree=DEGREE)\n",
    "Z_pred\n",
    "\n",
    "y_pred = model.predict(Z_pred)\n",
    "\n",
    "# Plot prediction and training set\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "ax.plot(x_pred, y_pred)\n",
    "ax.scatter(x, y);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute paths\n",
    "\n",
    "alphas = np.logspace(-2, 5, 50)\n",
    "\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    ridge = sklearn.linear_model.Ridge(alpha=a, fit_intercept=False)\n",
    "    ridge.fit(Z, y)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "# Display results\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "#ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Ridge coefficients as a function of the regularization')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{\\theta^*} = \\left( \\lambda \\boldsymbol{I}_p + \\boldsymbol{X}^T \\boldsymbol{X} \\right)^{-1} \\boldsymbol{X}^T \\boldsymbol{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(X, y, lambda_):\n",
    "    XX = np.dot(X.T, X)\n",
    "    I = lambda_ * np.eye(XX.shape[0])\n",
    "    Xy = np.dot(X.T, y)\n",
    "    invXX = np.linalg.inv(I + XX)\n",
    "\n",
    "    theta = np.dot(invXX, Xy)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = ridge_regression(Z, y, lambda_=6.)\n",
    "\n",
    "print(\"Coefs:\", theta)\n",
    "\n",
    "# Compute the model's prediction\n",
    "\n",
    "x_pred = np.linspace(x.min(), x.max(), 100)\n",
    "Z_pred = basis_expansion(x_pred, degree=DEGREE)\n",
    "\n",
    "y_pred = np.dot(Z_pred, theta)\n",
    "\n",
    "# Plot prediction and training set\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "ax.plot(x_pred, y_pred)\n",
    "ax.scatter(x, y);\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
